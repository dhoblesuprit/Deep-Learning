{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ECBM E4040 Assignment 2 - Task 1: Optimization\n",
    "\n",
    "In this task, we introduce multiple SGD-based optimization methods. As you have learned from the last assignment, SGD is an efficient method to update parameters. However, to make SGD perform well, we need to find an appropriate learning rate and a good initial value. Otherwise, the network will get stuck if learning rate is small, or it will diverge if the learning rate is too large. In reality, since we have no prior knowledge about the training data, it is not trivial to find a good learning rate manually. Also, when the network becomes deeper, for each layer we may need to set a different learning rate, and that will again increase the development workload. Obviously, this is not a good direction. \n",
    "\n",
    "Another common problem is the lack of sufficient training data. This can make our training get stuck when using the naive SGD method. \n",
    "\n",
    "So, **how to set a good learning rate?** You are going to experiment with **SGD with momentum**, **RMSProp**, **Adam** and make comparisons.\n",
    "\n",
    "All of these optimizers are adaptive learning rate methods. Here is a useful link: http://ruder.io/optimizing-gradient-descent/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "% matplotlib inline\n",
    "% load_ext autoreload\n",
    "% autoreload 2\n",
    "\n",
    "# Import modules\n",
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from ecbm4040.cifar_utils import load_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load CIFAR 10\n",
    "\n",
    "Here we use a small dataset with only 2500 samples to simulate \"lack-of-data\" situation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the raw CIFAR-10 data.\n",
    "X_train, y_train, X_test, y_test = load_data()\n",
    "X_val = X_train[:500,:]\n",
    "y_val = y_train[:500]\n",
    "X_train = X_train[500:2500,:]\n",
    "y_train = y_train[500:2500]\n",
    "\n",
    "mean_image = np.mean(X_train, axis=0).astype(np.float32)\n",
    "X_train = X_train.astype(np.float32) - mean_image\n",
    "X_val = X_val.astype(np.float32) - mean_image\n",
    "\n",
    "# We've vectorized the data for you. That is, we flatten the 32×32×3 images into 1×3072 Numpy arrays.\n",
    "print('Training data shape: ', X_train.shape)\n",
    "print('Training labels shape: ', y_train.shape)\n",
    "print('Validation data shape: ', X_val.shape)\n",
    "print('Validation labels shape: ', y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Implement Optimizers\n",
    "\n",
    "Here we provide an MLP code snippet for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from ecbm4040.neuralnets.mlp import MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original SGD (for comparison purpose only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from ecbm4040.optimizers import SGDOptim\n",
    "\n",
    "model = MLP(input_dim=3072, hidden_dims=[100, 100], num_classes=10, weight_scale=1e-3, l2_reg=0.0)\n",
    "optimizer = SGDOptim()\n",
    "hist_sgd = optimizer.train(model, X_train, y_train, X_val, y_val, \n",
    "                           num_epoch=30, batch_size=200, learning_rate=1e-2, learning_decay=0.95, \n",
    "                           verbose=False, record_interval=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGD + Momentum\n",
    "\n",
    "<span style=\"color:red\">__TODO:__</span> Edit **SGDmomentumOptim** in __./ecbm4040/optimizers.py__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from ecbm4040.optimizers import SGDmomentumOptim\n",
    "\n",
    "model = MLP(input_dim=3072, hidden_dims=[100, 100], num_classes=10, l2_reg=0.0, weight_scale=1e-3)\n",
    "optimizer = SGDmomentumOptim(model, momentum=0.8)\n",
    "hist_sgd_momentum = optimizer.train(model, X_train, y_train, X_val, y_val, \n",
    "                                         num_epoch=30, batch_size=200, learning_rate=1e-2, \n",
    "                                         learning_decay=0.95, verbose=False, record_interval=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RMSprop\n",
    "\n",
    "<span style=\"color:red\">__TODO:__</span> Edit **RMSpropOptim** in **./ecbm4040/optimizers.py**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from ecbm4040.optimizers import RMSpropOptim\n",
    "\n",
    "model = MLP(input_dim=3072, hidden_dims=[100, 100], num_classes=10, l2_reg=0.0, weight_scale=1e-3)\n",
    "optimizer = RMSpropOptim(model)\n",
    "hist_rmsprop = optimizer.train(model, X_train, y_train, X_val, y_val, \n",
    "                               num_epoch=30, batch_size=200, learning_rate=1e-5, \n",
    "                               learning_decay=0.95, verbose=False, record_interval=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adam\n",
    "\n",
    "<span style=\"color:red\">__TODO:__</span> Edit **AdamOptim** in **./ecbm4040/optimizers.py**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from ecbm4040.optimizers import AdamOptim\n",
    "\n",
    "model = MLP(input_dim=3072, hidden_dims=[100, 100], num_classes=10, l2_reg=0.0, weight_scale=1e-3)\n",
    "optimizer = AdamOptim(model)\n",
    "hist_adam = optimizer.train(model, X_train, y_train, X_val, y_val, \n",
    "                            num_epoch=30, batch_size=200, learning_rate=1e-3, \n",
    "                            learning_decay=0.95, verbose=False, record_interval=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Comparison\n",
    "\n",
    "<span style=\"color:red\">__TODO:__</span> Run the following cells, which plot the loss curves of different optimizers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss_hist_sgd, train_acc_hist_sgd, val_acc_hist_sgd = hist_sgd\n",
    "loss_hist_momentum, train_acc_hist_momentum, val_acc_hist_momentum = hist_sgd_momentum\n",
    "loss_hist_rmsprop, train_acc_hist_rmsprop, val_acc_hist_rmsprop = hist_rmsprop\n",
    "loss_hist_adam, train_acc_hist_adam, val_acc_hist_adam = hist_adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plot training error curve of optimizers\n",
    "plt.plot(loss_hist_sgd, label=\"sgd\")\n",
    "plt.plot(loss_hist_momentum, label=\"momentum\")\n",
    "plt.plot(loss_hist_rmsprop, label=\"rmsprop\")\n",
    "plt.plot(loss_hist_adam, label=\"adam\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plot training accuracy curve of optimizers\n",
    "plt.plot(train_acc_hist_sgd, label=\"sgd\")\n",
    "plt.plot(train_acc_hist_momentum, label=\"momentum\")\n",
    "plt.plot(train_acc_hist_rmsprop, label=\"rmsprop\")\n",
    "plt.plot(train_acc_hist_adam, label=\"adam\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plot validation accuracy curve of optimizers\n",
    "plt.plot(val_acc_hist_sgd, label=\"sgd\")\n",
    "plt.plot(val_acc_hist_momentum, label=\"momentum\")\n",
    "plt.plot(val_acc_hist_rmsprop, label=\"rmsprop\")\n",
    "plt.plot(val_acc_hist_adam, label=\"adam\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<span style=\"color:red\">__TODO:__</span> Describe your results, and discuss your understandings of these optimizers, such as their advantages/disadvantages and when to use them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: **[fill in here]**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
