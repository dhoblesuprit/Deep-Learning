{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ECBM E4040 Assignment 2 - Task 1: Regularizations\n",
    "\n",
    "In this task, you are going to experiment with two popular regularization methods. \n",
    "\n",
    "** Dropout:** In the last assignment, you have trained a shallow network and everything looked fine. However, when the network becomes larger and deeper, it will also become harder to train. The first potential problem is over-fitting, that is, the network overreacts to noise or random errors of the training data, while it fails to detect the underlying distribution pattern. It is more likely to occur when the model becomes complex and contains more trainable parameters. Dropout is a well-known method that can eliminate such effects. The core idea behind it is quite simple, ie., rather than updating all trainable parameters each time, it randomly selects a subset of parameters to update and keeps other parameters unaltered.\n",
    "\n",
    "\n",
    "\n",
    "** Batch normalization:** Batch normalization makes it possible to train a deep network. When a network becomes deeper, the distribution of hidden neurons' values will also shift greatly, and that is one reason that makes it hard to train a deep neural network. Machine learning teaches that normalization is a good preprocessing method to deal with such a problem. So  batch normalization deploys a similar idea in the neural network. It re-normalizes the hidden values of each layer before transfering values to the next layer.\n",
    "\n",
    "\n",
    "* References\n",
    "    * https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf\n",
    "    * https://arxiv.org/pdf/1502.03167.pdf\n",
    "    * https://www.tensorflow.org/api_docs/python/tf/contrib/layers/batch_norm\n",
    "    * https://github.com/tensorflow/tensorflow/blob/r1.3/tensorflow/contrib/layers/python/layers/layers.py\n",
    "    * https://www.tensorflow.org/api_docs/python/tf/nn/batch_normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "% matplotlib inline\n",
    "% load_ext autoreload\n",
    "% autoreload 2\n",
    "\n",
    "# Import modules\n",
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from ecbm4040.cifar_utils import load_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the raw CIFAR-10 data.\n",
    "X_train, y_train, X_test, y_test = load_data()\n",
    "X_val = X_train[:500,:]\n",
    "y_val = y_train[:500]\n",
    "X_train = X_train[500:2500,:]\n",
    "y_train = y_train[500:2500]\n",
    "\n",
    "mean_image = np.mean(X_train, axis=0).astype(np.float32)\n",
    "X_train = X_train.astype(np.float32) - mean_image\n",
    "X_val = X_val.astype(np.float32) - mean_image\n",
    "\n",
    "# We've vectorized the data for you. That is, we flatten the 32×32×3 images into 1×3072 Numpy arrays.\n",
    "print('Training data shape: ', X_train.shape)\n",
    "print('Training labels shape: ', y_train.shape)\n",
    "print('Validation data shape: ', X_val.shape)\n",
    "print('Validation labels shape: ', y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement dropout foward function\n",
    "\n",
    "<span style=\"color:red\"><strong>TODO:</strong></span> Edit functions **dropout_forward** in **./ecbm4040/reg_funcs.py**. If the code is running correctly, you will see that the outputs of checking code should be close to each other. Also, if the function is correct, then the output mean should be close to each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_in = np.random.randn(500, 500) + 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from ecbm4040.reg_funcs import dropout_forward\n",
    "from ecbm4040.reg_funcs import dropout_backward\n",
    "\n",
    "p = 0.5\n",
    "dropout_config = {\"enabled\": True, \"keep_prob\": p}\n",
    "# feedforward\n",
    "out, cache = dropout_forward(x=x_in, dropout_config=dropout_config, mode=\"train\")\n",
    "out_test, _ = dropout_forward(x=x_in, dropout_config=dropout_config, mode=\"test\") \n",
    "# backward\n",
    "dout = np.ones_like(x_in)\n",
    "dx = dropout_backward(dout, cache)\n",
    "#######################################\n",
    "# Checking code. Don't change it.     #\n",
    "#######################################\n",
    "# Check forward correctness\n",
    "print(\"mean_of_input = {}\".format(p*np.mean(x_in)))\n",
    "print(\"mean_of_out = {}\".format(np.mean(out)))\n",
    "print(\"mean_of_out_test = {}\".format(np.mean(out_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiments with MLP\n",
    "\n",
    "<span style=\"color:red\"><strong>TODO:</strong></span>\n",
    "\n",
    "1. Add dropout into MLP. Please review **loss**, **predict** function of class **MLP** in **./ecbm4040/neuralnets/mlp.py**, and understand how **dropout** is added into MLP.\n",
    "\n",
    "2. Customize your own MLP network.Then, train networks with different **keep_prob**, like 0.1, 0.5, 0.7, 0.9, 1. If **keep_prob**==1, then the network become the normal MLP without dropout.\n",
    "\n",
    "3. Plot the loss, train accuracy and valid accuracy curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from ecbm4040.neuralnets.mlp import MLP \n",
    "from ecbm4040.optimizers import AdamOptim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Here is an example on how to collect loss and accuracy info\n",
    "dropout_config = {\"enabled\":True, \"keep_prob\": 1}\n",
    "model = MLP(input_dim=3072, hidden_dims=[200], num_classes=10, \n",
    "            weight_scale=1e-3, l2_reg=0.0, dropout_config=dropout_config)\n",
    "optimizer = AdamOptim(model)\n",
    "# hist_no_dropout contains loss, train acc and valid acc history.\n",
    "hist_no_dropout = optimizer.train(model, X_train, y_train, X_val, y_val, \n",
    "                           num_epoch=20, batch_size=100, learning_rate=1e-3, learning_decay=0.95, \n",
    "                           verbose=False, record_interval = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# small retention rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# keep prob 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# keep prob 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# keep prob 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plot loss curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plot train acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plot val acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\"><strong>TODO:</strong></span> Describe what you find in this dropout experiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: **[fill in here]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Batch Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement batch normalization foward function\n",
    "\n",
    "<span style=\"color:red\"><strong>TODO:</strong></span> Edit functions **bn_forward** in **./ecbm4040/reg_funcs.py**. If the code is running correctly, you will see that the outputs of checking code should be close to each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from ecbm4040.reg_funcs import bn_forward\n",
    "from ecbm4040.reg_funcs import bn_backward\n",
    "\n",
    "np.random.seed(4040)\n",
    "N, D, H1, H2 = 200, 10, 5, 3\n",
    "x_in = np.random.randn(N, D)\n",
    "w1 = np.random.randn(D,H1)\n",
    "w2 = np.random.randn(H1,H2)\n",
    "a2 = np.maximum(x_in.dot(w1),0).dot(w2)\n",
    "\n",
    "# Before batch normalization\n",
    "print(\"mean of a2: \", np.mean(a2, axis=0))\n",
    "print(\"var of a2: \", np.var(a2, axis=0))\n",
    "\n",
    "# Test \"train mode\" of forward function\n",
    "# After batch normalization, the mean should be close to zero and var should be close to one. \n",
    "bn_config = {\"epsilon\":1e-5, \"decay\":0.9}\n",
    "gamma = np.ones(H2)\n",
    "beta = np.zeros(H2)\n",
    "a2_bn, _ = bn_forward(a2, gamma, beta, bn_config, \"train\")\n",
    "print(\"(train) mean of a2_bn: \", np.mean(a2_bn, axis=0))\n",
    "print(\"(train) var of a2_bn: \", np.var(a2_bn, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Test \"moving average\" and \"test mode\" of forward function\n",
    "# Then you are going to run the forward function under \"training mode\" for several times, \n",
    "# and the moving mean and moving var will be close to the real mean and var of the input data.\n",
    "# Next, run the forward function under \"test\" mode and you will see that the mean and var of its \n",
    "# output will be also close to gamma, beta that you have set before.\n",
    "\n",
    "bn_config = {\"epsilon\":1e-5, \"decay\":0.9}\n",
    "gamma = np.ones(H2)\n",
    "beta = np.zeros(H2)\n",
    "\n",
    "# collect_data: for calculating real mean and var of a2 later.\n",
    "collect_data = a2\n",
    "for _ in range(100):\n",
    "    x_in = np.random.randn(N, D)\n",
    "    a2 = np.maximum(x_in.dot(w1),0).dot(w2)\n",
    "    collect_data = np.concatenate((collect_data, a2), axis=0)\n",
    "    bn_forward(a2, gamma, beta, bn_config, \"train\")\n",
    "\n",
    "# compare moving_mean and moving_var with real mean and var.\n",
    "# You should see that they are close to each other.\n",
    "print(\"real mean of data: \", np.mean(collect_data, axis=0))\n",
    "print(\"(real var of data: \", np.var(collect_data, axis=0))\n",
    "print(\"moving mean of data: \", bn_config[\"moving_mean\"])\n",
    "print(\"(moving var of data: \", bn_config[\"moving_var\"])\n",
    "\n",
    "# \"test mode\" of forward function\n",
    "# After bn_forward, the mean and var of output should be kind of close to gamma and beta.\n",
    "x_in = np.random.randn(N, D)\n",
    "a2 = np.maximum(x_in.dot(w1),0).dot(w2)\n",
    "print(\"*\"*80)\n",
    "print(\"mean of a2: \", np.mean(a2, axis=0))\n",
    "print(\"var of a2: \", np.var(a2, axis=0))\n",
    "a2_bn, _ = bn_forward(a2, gamma, beta, bn_config, \"test\")\n",
    "print(\"(test) mean of a2_bn: \", np.mean(a2_bn, axis=0))\n",
    "print(\"(test) var of a2_bn: \", np.var(a2_bn, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# After verifying the forward function and save the bn_config.\n",
    "x_in = np.random.randn(N, D)\n",
    "a2 = np.maximum(x_in.dot(w1),0).dot(w2)\n",
    "da2_bn = np.ones_like(a2)\n",
    "# Test backward function with tensorflow\n",
    "#bn_config = {\"eps\":1e-5, \"decay\":0.9, \"moving_mean\":moving_mean, \"moving_var\":moving_var}\n",
    "gamma = np.ones(H2)\n",
    "beta = np.zeros(H2)\n",
    "a2_bn, cache = bn_forward(a2, gamma, beta, bn_config, \"test\")\n",
    "da2, dgamma, dbeta = bn_backward(da2_bn, cache)\n",
    "\n",
    "# Checking code below.\n",
    "a2_tf = tf.Variable(a2)\n",
    "eps = bn_config[\"epsilon\"]\n",
    "mean_tf = bn_config[\"moving_mean\"]\n",
    "var_tf = bn_config[\"moving_var\"]\n",
    "gamma_tf = tf.Variable(gamma)\n",
    "beta_tf = tf.Variable(beta)\n",
    "a2_bn_tf = tf.nn.batch_normalization(a2_tf, mean_tf, var_tf, beta_tf, gamma_tf, eps)\n",
    "# derive gradients\n",
    "da2_tf = tf.gradients(a2_bn_tf, a2_tf)\n",
    "dgamma_tf = tf.gradients(a2_bn_tf, gamma_tf)\n",
    "dbeta_tf = tf.gradients(a2_bn_tf, beta_tf)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "tf_output = (a2_bn_tf, da2_tf[0], dgamma_tf[0], dbeta_tf[0])\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    a2_bn_check, da2_check, dgamma_check, dbeta_check = sess.run(tf_output)\n",
    "    \n",
    "# Make comparison\n",
    "print(\"Is a2_bn correct? {}\".format(np.allclose(a2_bn, a2_bn_check)))\n",
    "print(\"Is da2 correct? {}\".format(np.allclose(da2, da2_check)))\n",
    "print(\"Is dgamma correct? {}\".format(np.allclose(dgamma, dgamma_check)))\n",
    "print(\"Is dbeta correct? {}\".format(np.allclose(dbeta, dbeta_check)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiments with MLP\n",
    "\n",
    "<span style=\"color:red\"><strong>TODO:</strong></span>\n",
    "\n",
    "1. Add batch normalization into MLP. Please understand **loss**, **predict** function of class **MLP** in **./ecbm4040/neuralnets/mlp.py**\n",
    "\n",
    "2. First create a shallow MLP like two-layer network like [100,10]. And train it without/with batch normalization. Plot the loss, train accuracy and valid accuracy curve.\n",
    "\n",
    "3. Then, create a 5-layer MLP network like [100,50,50,50,10] and train the network with/without batch normalization. Again, Plot the loss, train accuracy and valid accuracy curve.\n",
    "\n",
    "4. Make a comparison and describe what you have found in this experiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Experiment on shallow MLP** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Build a two-layer netowrk without batch normalization.\n",
    "# Here is a demo.\n",
    "use_bn = False\n",
    "model = MLP(input_dim=3072, hidden_dims=[100], num_classes=10, \n",
    "            weight_scale=1e-3, l2_reg=0.0, use_bn=use_bn)\n",
    "optimizer = AdamOptim(model)\n",
    "# hist_shallow_no_bn contains loss, train acc and valid acc history.\n",
    "hist_shallow_no_bn = optimizer.train(model, X_train, y_train, X_val, y_val, \n",
    "                           num_epoch=50, batch_size=100, learning_rate=1e-3, learning_decay=0.95, \n",
    "                           verbose=False, record_interval = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Build a two-layer netowrk with batch normalization. Remember to \"use_bn\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plot both results together and make a comparison. \n",
    "title_name = [\"loss\", \"train acc\", \"val acc\"]\n",
    "_, axarr = plt.subplots(1,3, figsize=(15,5))\n",
    "for i in range(3):\n",
    "    axarr[i].plot(hist_shallow_no_bn[i], label=\"no_bn\")\n",
    "    axarr[i].plot(hist_shallow_bn[i], label=\"bn\")\n",
    "    axarr[i].legend(), axarr[i].set_title(title_name[i])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Experiment on deep MLP**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Build a deep 5-layer netowrk without batch normalization. Remember to \"use_bn\".\n",
    "use_bn = False\n",
    "model = MLP(input_dim=3072, hidden_dims=[100, 50, 50, 50], num_classes=10, \n",
    "            weight_scale=1e-3, l2_reg=0.0, use_bn=use_bn)\n",
    "optimizer = AdamOptim(model)\n",
    "# hist_no_dropout contains loss, train acc and valid acc history.\n",
    "hist_deep_no_bn = optimizer.train(model, X_train, y_train, X_val, y_val, \n",
    "                           num_epoch=50, batch_size=100, learning_rate=1e-3, learning_decay=0.95, \n",
    "                           verbose=False, record_interval = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Build a deep 5-layer netowrk with batch normalization. Remember to \"use_bn\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plot both results together and make a comparison. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\"><strong>TODO:</strong></span> Describe what you find in this experiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: **[fill in here]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Dropout + Batch Normalization (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a deep netowrk with both dropout and batch normalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference\n",
    "* Weight Normalization: A Simple Reparameterization to Accelerate Training of Deep Neural Networks https://arxiv.org/abs/1602.07868\n",
    "* Highway netowrk https://arxiv.org/pdf/1505.00387.pdf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
