{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2: Recurrent Attention Model (RAM)\n",
    "\n",
    "Reference: https://papers.nips.cc/paper/5542-recurrent-models-of-visual-attention.pdf\n",
    "\n",
    "Useful links:\n",
    "https://medium.com/towards-data-science/visual-attention-model-in-deep-learning-708813c2912c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Notebook auto reloads code. (Ref: http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython)\n",
    "% load_ext autoreload\n",
    "% autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load MNIST data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=False)\n",
    "\n",
    "print(\"number of training samples: \", mnist.train.num_examples)\n",
    "print(\"number of validation samples: \", mnist.validation.num_examples)\n",
    "print(\"number of test samples: \", mnist.test.num_examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2, Part 1: RAM networks\n",
    "\n",
    "Recurrent attention model (RAM) is a model which processes inputs sequentially. For example, in the image classification problem, rather than using the whole image as input, RAM only takes a look at the small patch of the image at each step. RAM itself can learn what/where it should pay attention to, depending on which task it is executing. The core network consists of three parts: **glimpse net**, **rnn net**, and **location net/action net**. \n",
    "\n",
    "As shown in the following figure, \n",
    "\n",
    "* Glimpse Net: shown in A) and B) in the figure. It includes a glimpse sensor which extracts a small patch from the original image, and a glimpse net which combines both glimpse info and location info together with fully connected network and $g_t$ output vector.\n",
    "\n",
    "* RNN Net: takes $g_t$ as input, passes it through a one-layer rnn network, and outputs the hidden states $h_t$ of the rnn cell.\n",
    "\n",
    "* Location Net: uses $h_t$ to estimate the next location $l_t$ for the glimpse.\n",
    "\n",
    "* Action/Classification Net: uses the last $h_t$ set as features to classify the label of a digit image.\n",
    "\n",
    "* At each step, $l_t$ is fed back to the glimpse net, to get the next $g_{t+1}$ for the RNN, as shown in C).\n",
    "\n",
    "![ram](./img/whole_net.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### step 1: Glimpse network\n",
    "\n",
    "<span style=\"color:red\">TODO:</span> Complete **glimpse_sensor**, and **\\__call__** functions of the **GlimpseNet class** in the **ecbm4040/ram/networks.py**.\n",
    "\n",
    "* Glimpse sensor/Retina and location encodings: The retina encoding ρ(x, l) extracts k square patches centered at location l, with the first patch being [glimpse_win × glimpse_win] pixels in size, and each successive patch having twice the width of the previous. The k patches are then all resized to [glimpse_win, glimpse_win] and concatenated. Glimpse locations l were encoded as real-valued (x, y) coordinates with (0, 0) being the center of the image x, and (−1, −1) being the top left corner of x. So the value of x or y is between -1 and 1.\n",
    "\n",
    "* Glimpse net: it is one kind of a MLP network.\n",
    "  * $hl = ReLU( Linear(l) )$\n",
    "  * $hg = ReLU(Linear(ρ(x, l)))$\n",
    "  * $g = ReLU(Linear(hl) + Linear(hg))$\n",
    "\n",
    "![glimpse_net](./img/glimpse_net.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### step 2: Location network\n",
    "\n",
    "<span style=\"color:red\">TODO:</span> Finish **\\__call__** function of **LocNet class** in **ecbm4040/ram/networks.py**.\n",
    "\n",
    "* Location net:\n",
    "  * ${E}[l_t] = Linear(h_t)$\n",
    "  * Gaussian stochastic policy: $l_t \\sim N(E[l_t], \\sigma^2)$, next location for glimpse is sampled from a gaussian distribution with $E[l_t]$ as mean and a fixed $\\sigma$ as std. deviation.\n",
    "  * Here $\\sigma$ is a fixed number.\n",
    "  * **Location net is a stochastic net.** In the paper, it uses REINFORCE to train it. You can find more details in the following link. https://medium.com/towards-data-science/visual-attention-model-in-deep-learning-708813c2912c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### step 3: Action network (classification in this experiment)\n",
    "\n",
    "<span style=\"color:red\">TODO:</span> Complete **\\__call__** function of **ActionNet class** in **ecbm4040/ram/networks.py**.\n",
    "\n",
    "* Action net:\n",
    "  * $a = Linear(h_T)$, here $h_T$ is the last output of the rnn network.\n",
    "  * $softmax\\_a = softmax(a)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### step 4: Core RNN network\n",
    "\n",
    "<span style=\"color:red\">TODO:</span> Complete **core_rnn_net** function in **ecbm4040/ram/model.py**.\n",
    "\n",
    "In this experiment, we use the LSTM cell.\n",
    "\n",
    "In the core rnn net, \n",
    "* First, define the LSTM cell.\n",
    "* Then, initialize the init state $g$.\n",
    "* Build a loop function which keeps feeding new glimpse into the LSTM cell.\n",
    "* Output hidden states to the location net or action net.\n",
    "\n",
    "![core_rnn_net](./img/core_rnn_net.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2, Part 2: Training "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Goal \n",
    "\n",
    "Train the glimpse net, core RNN net, and action/location net."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hybrid Supervised Loss\n",
    "\n",
    "The code is provided. It is in file **ecbm4040/ram/loss.py** and **model** function in **ecbm4040/ram/model.py**.\n",
    "\n",
    "To train RAM, we need to define the loss function. Originally, RAM can be perceived as an agent trying to solve a Partially Observable Markov Decision Process (POMDP) problem. At each step, after it takes an action like choosing the glimpse location, the agent will receive a reward signal, so its main goal is to maximize the total sum of reward signals. In the case of object recognition, for example, $R$ = 1 if the object is classified correctly after T steps, and 0 otherwise. T is the total number of glimpses except for the initial random glimpse.\n",
    "\n",
    "To **maximize** this reward signal, the objective function is defined as Eq. (1) in the paper.\n",
    "\n",
    "$$J = \\frac{1}{M}\\sum_{i=1}^M \\sum_{t=1}^T log(\\pi(l_t^i\\ |\\ s_{1:t}^i;\\theta)) \\times R^i$$ \n",
    "\n",
    "Here, $\\theta$ means all trainable parameters in the network. $R_i$ is the reward signal of sample $i$. $M$ implies that it uses **Monte Carlo** sampling to estimate the loss - this is a famous method used in reinforcement learning (RL). LocNet is a stochastic net, where the output next_loc is sampled from a gaussian distribution. This makes the net indifferentiable, and we can not use an  ordinary back-propagation method to update the parameters. So we need to use Monte Carlo sampling, taking  the average loss from M samples as the estimation of loss, and applying back-propagation via this loss. Also, during back-propagation we need to avoid taking this stochastic probability into account. Stochastic LocNet uses REINFORCE for training.\n",
    "\n",
    "However, this loss may have high variance. To reduce the variance, it uses a **baseline network**. Then, the objective function becomes\n",
    "\n",
    "$$J = \\frac{1}{M}\\sum_{i=1}^M \\sum_{t=1}^T log(\\pi(l_t^i\\ |\\ s_{1:t}^i;\\theta)) \\times (R_t^i - b_t^i)$$\n",
    "\n",
    "where $R_t^i$ is always equal to $R^i$ based on the reward definition above. $b^i$ is an estimation of $E[R_t]$, and $R_t$ here only relies on its state value $h_t$ and is independent of the LocNet and its action value $l_t$. In practice, it uses another baseline network to estimate this value. And the baseline network is defined as a single-layer fully-connected network with the goal of reducing the **squared error between $R_t^i$ and $b_t^i$**. Also, remember that with respect to $J$, $b_t^i$ is a constant value, which means that $b_t^i$ should not be considered in backpropagtion through this part of the network.\n",
    "\n",
    "**Hybrid supervised loss: **\n",
    "As mentioned in the paper, the algorithm described above allows us to train the agent when the “best” actions are unknown, and the learning signal is only provided via the reward. For instance, we may not know a priori which sequence of fixations provides the most information about an unknown image, but the total reward at the end of an episode will give us an indication whether the tried sequence was good or bad.\n",
    "\n",
    "However, in some situations we do know the correct action to take: For instance, in the object detection task the agent has to output the label of the object as the final action. For training images this label will be known, and we can directly optimize the policy to **output the correct label associated with a training image at the end of an observation sequence**. We follow the approach for the classification problems and optimize the **cross entropy loss** to train the action network fa and backpropagate the gradients through the core and glimpse networks.\n",
    "\n",
    "so the final hybrid loss is defined as,\n",
    "\n",
    "$$Hybrid\\ Loss = -J + \\frac{1}{M}\\sum_{i=1}^M cross\\_entropy(softmax\\_a) + \\frac{1}{M}\\sum_{i=1}^M \\sum_{t=1}^T (R^i-b_t^i)^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer and Back-propagation\n",
    "\n",
    "Code is given in **ecbm4040/ram/model.py**. Study and try to understand the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# network configuration\n",
    "config = {\n",
    "    # input configuration\n",
    "    \"image_size\": 28,\n",
    "    \"num_channels\": 1,\n",
    "    # network settings\n",
    "    ## glimpse\n",
    "    \"glimpse_win\": 12,\n",
    "    \"glimpse_scale\": 1,\n",
    "    \"hg_dim\": 128,\n",
    "    \"hl_dim\": 128,\n",
    "    \"g_dim\": 256,\n",
    "    ## rnn\n",
    "    \"num_glimpses\": 6,\n",
    "    \"cell_dim\": 256,\n",
    "    ## location\n",
    "    \"loc_dim\": 2,\n",
    "    \"loc_std\": 0.1, # you can try different std\n",
    "    \"use_sample\": True,\n",
    "    ## action/classification\n",
    "    \"num_classes\": 10\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# training configuration\n",
    "train_cfg = {\n",
    "    \"max_grad_norm\": 5.,\n",
    "    \"lr_init\": 1e-4,\n",
    "    \"lr_min\": 1e-5,\n",
    "    \"decay_rate\": 0.95,\n",
    "    \"num_epochs\": 15, # you should try more epoch\n",
    "    \"num_train\": mnist.train.num_examples,\n",
    "    \"batch_size\": 32,\n",
    "    \"eval_size\": 1000,\n",
    "    # monte carlo sampling\n",
    "    \"M\": 10\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from ecbm4040.ram.model import model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "out = model(config, train_cfg, reuse_core=False, reuse_action=False)\n",
    "images_ph, labels_ph, hybrid_loss, J, cross_ent, b_mse, r_avg, correct_num, lr, train_step, loc_means, loc_samples = out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this cell, to verify that the program works well, before going to training.\n",
    "with tf.Session() as sess:\n",
    "    images = mnist.train.images[:10,:]\n",
    "    labels = mnist.train.labels[:10]\n",
    "    images = images.reshape((10, 28, 28, 1))\n",
    "    \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    test_J, test_ent, test_bmse, test_r = sess.run([J, cross_ent, b_mse, r_avg],\n",
    "                                           feed_dict={images_ph: images,labels_ph: labels})\n",
    "print(\"outputs: J={:5f}, cross_ent={:5f}, baseline_mse={:5f}, reward_avg={:5f}\".format(test_J, test_ent, test_bmse, test_r))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Experiments\n",
    "\n",
    "It is recommended to use a GPU to complete the following experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load functions to visualize the glimpse path.\n",
    "from ecbm4040.ram.utils import glimpse_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original 28x28 MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display some samples\n",
    "num_display_samples = 4\n",
    "images, labels = mnist.train.next_batch(num_display_samples)\n",
    "images = images.reshape((num_display_samples, 28, 28, 1))\n",
    "f, axarr = plt.subplots(1, num_display_samples, figsize=(4*num_display_samples,4))\n",
    "for i in range(num_display_samples):\n",
    "    axarr[i].imshow(np.squeeze(images[i,:,:,:]), cmap=\"gray\")\n",
    "    axarr[i].set_title(labels[i])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Build the model:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "config[\"use_sample\"] = True\n",
    "tf.reset_default_graph()\n",
    "out = model(config, train_cfg, reuse_core=False, reuse_action=False)\n",
    "images_ph, labels_ph, hybrid_loss, J, cross_ent, b_mse, r_avg, correct_num, lr, train_step, loc_means, loc_samples = out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train the model:** \n",
    "\n",
    "* You should reach <span style=\"color:red\">90%</span> validation & test acc in this experiment.\n",
    "* This network may be sentitive to its init weigths.\n",
    "* Overfitting may happen during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# train\n",
    "M = train_cfg[\"M\"]\n",
    "num_epochs = train_cfg[\"num_epochs\"]\n",
    "num_steps_per_epoch = train_cfg[\"num_train\"] // train_cfg[\"batch_size\"]\n",
    "eval_size = train_cfg[\"eval_size\"]\n",
    "# save\n",
    "saver = tf.train.Saver()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for e in range(num_epochs):\n",
    "        #####################################\n",
    "        #           training phase          #\n",
    "        #####################################\n",
    "        for i in range(num_steps_per_epoch):\n",
    "            images, labels = mnist.train.next_batch(train_cfg[\"batch_size\"])\n",
    "            images = images.reshape((train_cfg[\"batch_size\"], 28, 28, 1))\n",
    "            # Monte Carlo Estimation: duplicate M times, see Eqn (1) and (2) in paper\n",
    "            images = np.tile(images, [M, 1, 1, 1])\n",
    "            labels = np.tile(labels, [M])\n",
    "            # training\n",
    "            train_loss, train_J, train_ent, train_bmse, train_r, train_lr, _ = sess.run([hybrid_loss, J, cross_ent, b_mse, r_avg, lr, train_step],\n",
    "                                                   feed_dict={images_ph: images,labels_ph: labels})\n",
    "            # report progress\n",
    "            if i and i % 500 == 0:\n",
    "                print(\"epoch {} step {}: lr = {:.5f}\\treward = {:.4f}\\tloss = {:.4f}\".\n",
    "                      format(e+1, i, train_lr, train_r, train_loss))\n",
    "                print(\"epoch {} step {}: J = {:.5f}\\tcross_ent = {:.4f}\\tbaseline_mse = {:.4f}\".\n",
    "                      format(e+1, i, train_J, train_ent, train_bmse))\n",
    "                \n",
    "        #####################################\n",
    "        #         evaluation phase          #\n",
    "        #####################################\n",
    "        # validation set\n",
    "        val_correct_num = 0.0\n",
    "        for i in range(mnist.validation.num_examples//eval_size):\n",
    "            images, labels = mnist.validation.next_batch(eval_size)\n",
    "            images = images.reshape((eval_size, 28, 28, 1))\n",
    "            \n",
    "            val_correct_num += sess.run(correct_num, feed_dict={images_ph: images,labels_ph: labels})\n",
    "        val_acc = val_correct_num/mnist.validation.num_examples\n",
    "        print(\"------epoch {}: val_acc = {:.4f}\".format(e+1, val_acc))\n",
    "    \n",
    "    #####################################\n",
    "    #            save model             #\n",
    "    #####################################\n",
    "    # Save the variables to disk.\n",
    "    if not os.path.exists(\"./tmp/\"):\n",
    "        os.mkdir(\"./tmp/\")\n",
    "    save_path = saver.save(sess, \"./tmp/model_28_15.ckpt\")\n",
    "    print(\"Model saved in file: %s\" % save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()\n",
    "# evaluation: output val_acc and test_acc\n",
    "eval_size = train_cfg[\"eval_size\"]\n",
    "with tf.Session() as sess:\n",
    "    # Restore variables from disk.\n",
    "    saver.restore(sess, \"./tmp/model_28_15.ckpt\")\n",
    "    print(\"Model restored.\")\n",
    "    # validation set\n",
    "    val_correct_num = 0.0\n",
    "    for i in range(mnist.validation.num_examples//eval_size):\n",
    "        images = mnist.validation.images[i*eval_size:(i+1)*eval_size]\n",
    "        labels = mnist.validation.labels[i*eval_size:(i+1)*eval_size]\n",
    "        images = images.reshape((eval_size, 28, 28, 1))\n",
    "\n",
    "        val_correct_num += sess.run(correct_num, feed_dict={images_ph: images,labels_ph: labels})\n",
    "    val_acc = val_correct_num/mnist.validation.num_examples\n",
    "    print(\"val_acc = {:.4f}\".format(val_acc))\n",
    "\n",
    "    # test set\n",
    "    test_correct_num = 0.0\n",
    "    for i in range(mnist.test.num_examples//eval_size):\n",
    "        images = mnist.test.images[i*eval_size:(i+1)*eval_size]\n",
    "        labels = mnist.test.labels[i*eval_size:(i+1)*eval_size]\n",
    "        images = images.reshape((eval_size, 28, 28, 1))\n",
    "\n",
    "        test_correct_num += sess.run(correct_num, feed_dict={images_ph: images,labels_ph: labels})\n",
    "    test_acc = test_correct_num/mnist.test.num_examples\n",
    "    print(\"test_acc = {:.4f}\".format(test_acc))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# display the glimpse path. You can use \"glimpse_path\" or create your own function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Translated 60x60 MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ecbm4040.ram.utils import translate_60_mnist\n",
    "\n",
    "# display some translated samples: all samples in one batch share the same translation transform.\n",
    "num_display_samples = 4\n",
    "images, labels = mnist.train.next_batch(num_display_samples)\n",
    "images = translate_60_mnist(images, image_size=28, num_channels=1)\n",
    "f, axarr = plt.subplots(1, num_display_samples, figsize=(4*num_display_samples,4))\n",
    "for i in range(num_display_samples):\n",
    "    axarr[i].imshow(np.squeeze(images[i,:,:,:]), cmap=\"gray\")\n",
    "    axarr[i].set_title(labels[i])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train the model:** set a larger glimpse window and add more scales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: evaluation, output val_acc and test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: display the glimpse path and glimpse patches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST pair addition (Optional)\n",
    "\n",
    "In this experiment, the task is to predict the sum of a pair of digits. So, remember to change the \"num_classes\" into 19. Note that running this experiment may take several hours with GPU.\n",
    "\n",
    "**Warning:** This experiment is really challenging. Based on previous experiments, it may take many hours to reach only 65% accuracy. So we make this part optional. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ecbm4040.ram.utils import mnist_addition_pair\n",
    "\n",
    "# display some samples: image -- a pair of digit; label -- the sum of two digits.\n",
    "num_display_samples = 4\n",
    "images, labels = mnist.train.next_batch(num_display_samples)\n",
    "images, labels = mnist_addition_pair(images, labels, image_size=28, num_channels=1)\n",
    "f, axarr = plt.subplots(1, num_display_samples, figsize=(4*num_display_samples,4))\n",
    "for i in range(num_display_samples):\n",
    "    axarr[i].imshow(np.squeeze(images[i,:,:,:]), cmap=\"gray\")\n",
    "    axarr[i].set_title(labels[i])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**train**: may need more glimpse to give the sensor more freedom and **a large enough M** to reduce the variance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: evaluation: output val_acc and test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: display the glimpse path and glimpse patches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other recommended reading\n",
    "\n",
    "[1] [Multiple object recognition with visual attention](https://arxiv.org/pdf/1412.7755.pdf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
